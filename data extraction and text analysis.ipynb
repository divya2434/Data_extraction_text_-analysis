{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826a5987",
   "metadata": {},
   "source": [
    "# Data Extraction and Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81e6ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c91a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (1.25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a20070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1d74c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\hp\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c1cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Use the full absolute file path\n",
    "df = pd.read_csv('C:\\\\Users\\\\HP\\\\Desktop\\\\excel project\\\\input.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a9c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sen =[url for url in df['URL']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9984df66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL_ID    float64\n",
       "URL        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "856b506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29815aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "706f6b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted article 123.0 and saved as extracted_articles\\123.0.txt\n",
      "Extracted article 321.0 and saved as extracted_articles\\321.0.txt\n",
      "Extracted article 2345.0 and saved as extracted_articles\\2345.0.txt\n",
      "Extracted article 4321.0 and saved as extracted_articles\\4321.0.txt\n",
      "Extracted article 432.0 and saved as extracted_articles\\432.0.txt\n",
      "Extracted article 2893.8 and saved as extracted_articles\\2893.8.txt\n",
      "Extracted article 3355.6 and saved as extracted_articles\\3355.6.txt\n",
      "Extracted article 3817.4 and saved as extracted_articles\\3817.4.txt\n",
      "Extracted article 4279.2 and saved as extracted_articles\\4279.2.txt\n",
      "Extracted article 4741.0 and saved as extracted_articles\\4741.0.txt\n",
      "Extracted article 5202.8 and saved as extracted_articles\\5202.8.txt\n",
      "Extracted article 5664.6 and saved as extracted_articles\\5664.6.txt\n",
      "Extracted article 6126.4 and saved as extracted_articles\\6126.4.txt\n",
      "Extracted article 6588.2 and saved as extracted_articles\\6588.2.txt\n",
      "Extracted article 7050.0 and saved as extracted_articles\\7050.0.txt\n",
      "Extracted article 7511.8 and saved as extracted_articles\\7511.8.txt\n",
      "Extracted article 7973.6 and saved as extracted_articles\\7973.6.txt\n",
      "Extracted article 8435.4 and saved as extracted_articles\\8435.4.txt\n",
      "Extracted article 8897.2 and saved as extracted_articles\\8897.2.txt\n",
      "Extracted article 9359.0 and saved as extracted_articles\\9359.0.txt\n",
      "Extracted article 9820.8 and saved as extracted_articles\\9820.8.txt\n",
      "Extracted article 10282.6 and saved as extracted_articles\\10282.6.txt\n",
      "Extracted article 10744.4 and saved as extracted_articles\\10744.4.txt\n",
      "Extracted article 11206.2 and saved as extracted_articles\\11206.2.txt\n",
      "Extracted article 11668.0 and saved as extracted_articles\\11668.0.txt\n",
      "Extracted article 12129.8 and saved as extracted_articles\\12129.8.txt\n",
      "Extracted article 12591.6 and saved as extracted_articles\\12591.6.txt\n",
      "Extracted article 13053.4 and saved as extracted_articles\\13053.4.txt\n",
      "Extracted article 13515.2 and saved as extracted_articles\\13515.2.txt\n",
      "Extracted article 13977.0 and saved as extracted_articles\\13977.0.txt\n",
      "Extracted article 14438.8 and saved as extracted_articles\\14438.8.txt\n",
      "Extracted article 14900.6 and saved as extracted_articles\\14900.6.txt\n",
      "Extracted article 15362.4 and saved as extracted_articles\\15362.4.txt\n",
      "Extracted article 15824.2 and saved as extracted_articles\\15824.2.txt\n",
      "Extracted article 16286.0 and saved as extracted_articles\\16286.0.txt\n",
      "Extracted article 16747.8 and saved as extracted_articles\\16747.8.txt\n",
      "Extracted article 17209.6 and saved as extracted_articles\\17209.6.txt\n",
      "Extracted article 17671.4 and saved as extracted_articles\\17671.4.txt\n",
      "Extracted article 18133.2 and saved as extracted_articles\\18133.2.txt\n",
      "Extracted article 18595.0 and saved as extracted_articles\\18595.0.txt\n",
      "Extracted article 19056.8 and saved as extracted_articles\\19056.8.txt\n",
      "Extracted article 19518.6 and saved as extracted_articles\\19518.6.txt\n",
      "Extracted article 19980.4 and saved as extracted_articles\\19980.4.txt\n",
      "Extracted article 20442.2 and saved as extracted_articles\\20442.2.txt\n",
      "Extracted article 20904.0 and saved as extracted_articles\\20904.0.txt\n",
      "Extracted article 21365.8 and saved as extracted_articles\\21365.8.txt\n",
      "Extracted article 21827.6 and saved as extracted_articles\\21827.6.txt\n",
      "Extracted article 22289.4 and saved as extracted_articles\\22289.4.txt\n",
      "Extracted article 22751.2 and saved as extracted_articles\\22751.2.txt\n",
      "Extracted article 23213.0 and saved as extracted_articles\\23213.0.txt\n",
      "Extracted article 23674.8 and saved as extracted_articles\\23674.8.txt\n",
      "Extracted article 24136.6 and saved as extracted_articles\\24136.6.txt\n",
      "Extracted article 24598.4 and saved as extracted_articles\\24598.4.txt\n",
      "Extracted article 25060.2 and saved as extracted_articles\\25060.2.txt\n",
      "Extracted article 25522.0 and saved as extracted_articles\\25522.0.txt\n",
      "Extracted article 25983.8 and saved as extracted_articles\\25983.8.txt\n",
      "Extracted article 26445.6 and saved as extracted_articles\\26445.6.txt\n",
      "Extracted article 26907.4 and saved as extracted_articles\\26907.4.txt\n",
      "Extracted article 27369.2 and saved as extracted_articles\\27369.2.txt\n",
      "Extracted article 27831.0 and saved as extracted_articles\\27831.0.txt\n",
      "Extracted article 28292.8 and saved as extracted_articles\\28292.8.txt\n",
      "Extracted article 28754.6 and saved as extracted_articles\\28754.6.txt\n",
      "Extracted article 29216.4 and saved as extracted_articles\\29216.4.txt\n",
      "Extracted article 29678.2 and saved as extracted_articles\\29678.2.txt\n",
      "Extracted article 30140.0 and saved as extracted_articles\\30140.0.txt\n",
      "Extracted article 30601.8 and saved as extracted_articles\\30601.8.txt\n",
      "Extracted article 31063.6 and saved as extracted_articles\\31063.6.txt\n",
      "Extracted article 31525.4 and saved as extracted_articles\\31525.4.txt\n",
      "Extracted article 31987.2 and saved as extracted_articles\\31987.2.txt\n",
      "Extracted article 32449.0 and saved as extracted_articles\\32449.0.txt\n",
      "Extracted article 32910.8 and saved as extracted_articles\\32910.8.txt\n",
      "Extracted article 33372.6 and saved as extracted_articles\\33372.6.txt\n",
      "Extracted article 33834.4 and saved as extracted_articles\\33834.4.txt\n",
      "Extracted article 34296.2 and saved as extracted_articles\\34296.2.txt\n",
      "Extracted article 34758.0 and saved as extracted_articles\\34758.0.txt\n",
      "Extracted article 35219.8 and saved as extracted_articles\\35219.8.txt\n",
      "Extracted article 35681.6 and saved as extracted_articles\\35681.6.txt\n",
      "Extracted article 36143.4 and saved as extracted_articles\\36143.4.txt\n",
      "Extracted article 36605.2 and saved as extracted_articles\\36605.2.txt\n",
      "Extracted article 37067.0 and saved as extracted_articles\\37067.0.txt\n",
      "Extracted article 37528.8 and saved as extracted_articles\\37528.8.txt\n",
      "Extracted article 37990.6 and saved as extracted_articles\\37990.6.txt\n",
      "Extracted article 38452.4 and saved as extracted_articles\\38452.4.txt\n",
      "Extracted article 38914.2 and saved as extracted_articles\\38914.2.txt\n",
      "Extracted article 39376.0 and saved as extracted_articles\\39376.0.txt\n",
      "Extracted article 39837.8 and saved as extracted_articles\\39837.8.txt\n",
      "Extracted article 40299.6 and saved as extracted_articles\\40299.6.txt\n",
      "Extracted article 40761.4 and saved as extracted_articles\\40761.4.txt\n",
      "Extracted article 41223.2 and saved as extracted_articles\\41223.2.txt\n",
      "Extracted article 41685.0 and saved as extracted_articles\\41685.0.txt\n",
      "Extracted article 42146.8 and saved as extracted_articles\\42146.8.txt\n",
      "Extracted article 42608.6 and saved as extracted_articles\\42608.6.txt\n",
      "Extracted article 43070.4 and saved as extracted_articles\\43070.4.txt\n",
      "Extracted article 43532.2 and saved as extracted_articles\\43532.2.txt\n",
      "Extracted article 43994.0 and saved as extracted_articles\\43994.0.txt\n",
      "Extracted article 44455.8 and saved as extracted_articles\\44455.8.txt\n",
      "Extracted article 44917.6 and saved as extracted_articles\\44917.6.txt\n",
      "Extracted article 45379.4 and saved as extracted_articles\\45379.4.txt\n",
      "Extracted article 45841.2 and saved as extracted_articles\\45841.2.txt\n",
      "Extracted article 46303.0 and saved as extracted_articles\\46303.0.txt\n",
      "Extracted article 46764.8 and saved as extracted_articles\\46764.8.txt\n",
      "Extracted article 47226.6 and saved as extracted_articles\\47226.6.txt\n",
      "Extracted article 47688.4 and saved as extracted_articles\\47688.4.txt\n",
      "Extracted article 48150.2 and saved as extracted_articles\\48150.2.txt\n",
      "Extracted article 48612.0 and saved as extracted_articles\\48612.0.txt\n",
      "Extracted article 49073.8 and saved as extracted_articles\\49073.8.txt\n",
      "Extracted article 49535.6 and saved as extracted_articles\\49535.6.txt\n",
      "Extracted article 49997.4 and saved as extracted_articles\\49997.4.txt\n",
      "Extracted article 50459.2 and saved as extracted_articles\\50459.2.txt\n",
      "Extracted article 50921.0 and saved as extracted_articles\\50921.0.txt\n",
      "Extracted article 51382.8 and saved as extracted_articles\\51382.8.txt\n",
      "Extracted article 51844.6 and saved as extracted_articles\\51844.6.txt\n",
      "Extracted article 52306.4 and saved as extracted_articles\\52306.4.txt\n",
      "Extracted article 52768.2 and saved as extracted_articles\\52768.2.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the Excel file\n",
    "input_file = \"input.csv\"\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Create a directory to store the extracted articles\n",
    "output_directory = \"extracted_articles\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Loop through the rows and extract articles\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the title and article text\n",
    "    title = soup.find('title').text\n",
    "    article_text = \"\\n\".join([p.text for p in soup.find_all('p')])\n",
    "\n",
    "    # Save the extracted article to a text file\n",
    "    output_filename = os.path.join(output_directory, f\"{url_id}.txt\")\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Title: {title}\\n\\n\")\n",
    "        f.write(article_text)\n",
    "\n",
    "    print(f\"Extracted article {url_id} and saved as {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70803f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting syllables\n",
      "  Downloading syllables-1.0.7-py3-none-any.whl (15 kB)\n",
      "Collecting cmudict<2.0.0,>=1.0.11 (from syllables)\n",
      "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
      "                                              0.0/939.3 kB ? eta -:--:--\n",
      "     --                                      71.7/939.3 kB 2.0 MB/s eta 0:00:01\n",
      "     ---------                              225.3/939.3 kB 2.3 MB/s eta 0:00:01\n",
      "     --------------                         368.6/939.3 kB 2.3 MB/s eta 0:00:01\n",
      "     -------------------                    491.5/939.3 kB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------            686.1/939.3 kB 2.4 MB/s eta 0:00:01\n",
      "     -----------------------------------    870.4/939.3 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 939.3/939.3 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata<6.0.0,>=5.1.0 (from syllables)\n",
      "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
      "Collecting importlib-resources<6.0.0,>=5.10.1 (from cmudict<2.0.0,>=1.0.11->syllables)\n",
      "  Downloading importlib_resources-5.13.0-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.11.0)\n",
      "Installing collected packages: importlib-resources, importlib-metadata, cmudict, syllables\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 6.0.0\n",
      "    Uninstalling importlib-metadata-6.0.0:\n",
      "      Successfully uninstalled importlib-metadata-6.0.0\n",
      "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 importlib-resources-5.13.0 syllables-1.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install syllables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08f60507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textual analysis process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from syllables import estimate\n",
    "\n",
    "# Load NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Read the CSV file containing extracted articles\n",
    "input_directory = \"extracted_articles\"\n",
    "df = pd.read_csv(\"input.csv\")  # Replace with your CSV file name\n",
    "\n",
    "# Create a DataFrame to store the analysis results\n",
    "output_df = pd.DataFrame(columns=[\"URL_ID\", \"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\", \"SUBJECTIVITY SCORE\",\n",
    "                                  \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
    "                                  \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\", \"WORD COUNT\",\n",
    "                                  \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"])\n",
    "\n",
    "# Loop through the rows and perform textual analysis\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    text_file_path = os.path.join(input_directory, f\"{url_id}.txt\")\n",
    "\n",
    "    with open(text_file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    blob = TextBlob(text)\n",
    "    positive_score = blob.sentiment.polarity if blob.sentiment.polarity > 0 else 0\n",
    "    negative_score = -blob.sentiment.polarity if blob.sentiment.polarity < 0 else 0\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "\n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "\n",
    "    # Calculate percentage of complex words\n",
    "    complex_word_count = sum(1 for word in words if word.lower() not in stop_words and len(word) > 2)\n",
    "    percentage_complex_words = (complex_word_count / len(words)) * 100\n",
    "\n",
    "    # Calculate FOG index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Calculate average number of words per sentence\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "\n",
    "    # Count personal pronouns\n",
    "    personal_pronouns = sum(1 for word in words if word.lower() in [\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"])\n",
    "\n",
    "    # Calculate average word length\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "\n",
    "    # Calculate syllables per word using the syllables module\n",
    "    syllable_per_word = sum(estimate(word) for word in words) / len(words)\n",
    "\n",
    "    # Calculate word count\n",
    "    word_count = len(words)\n",
    "\n",
    "    # Add the computed values to the output DataFrame\n",
    "    output_df.loc[index] = [url_id, positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "                            avg_sentence_length, percentage_complex_words, fog_index,\n",
    "                            avg_words_per_sentence, complex_word_count, word_count,\n",
    "                            syllable_per_word, personal_pronouns, avg_word_length]\n",
    "\n",
    "# Save the analysis results to an Excel file\n",
    "output_df.to_excel(\"text_analysis_results.xlsx\", index=False)\n",
    "\n",
    "print(\"Textual analysis process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d93ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
